{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjB-TdJOvNe2"
      },
      "source": [
        "# WRN-16-2\n",
        "\n",
        "**목표**: 71.6%를 달성했던 모델(WRN-16-2)에, 현재 가장 강력한 데이터 증강 기법 중 하나인 `RandAugment`를 적용하여 성능을 한계까지 끌어 올리는 것을 목표로합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT5o-0H9vNe6"
      },
      "source": [
        "## 1. 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rkMKeRbvNe6",
        "outputId": "97452298-5dda-457a-d12d-3abfd4a16485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLt7XaUbvNe7",
        "outputId": "eabba7a3-f858-412c-c6ed-024b39e98de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 임포트\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fpEvu8jvNe8",
        "outputId": "2e4aed50-6f24-408d-f272-6067cee784ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# 재현성을 위한 시드 고정\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "# GPU 장치 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7H2R117vNe8"
      },
      "source": [
        "## 2. 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qj6h1mHvNe9",
        "outputId": "c78ee8f4-743f-444b-8b11-36d79d8641a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data extracted to: /content/dataset\n",
            "Train data shape: (50000, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋 경로 설정\n",
        "zip_file_path = '/content/drive/MyDrive/2025-ai-challenge.zip'\n",
        "data_dir = '/content/dataset'\n",
        "\n",
        "# 데이터셋 압축 해제\n",
        "if os.path.exists(data_dir):\n",
        "    import shutil\n",
        "    shutil.rmtree(data_dir)\n",
        "os.makedirs(data_dir)\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(data_dir)\n",
        "\n",
        "print(f\"Data extracted to: {data_dir}\")\n",
        "data_subdir = '' # 하위 폴더 없음\n",
        "full_data_path = os.path.join(data_dir, data_subdir)\n",
        "\n",
        "# 데이터 로드\n",
        "train_data = np.load(os.path.join(full_data_path, 'trainset.npy'))\n",
        "train_labels = np.load(os.path.join(full_data_path, 'trainlabel.npy'))\n",
        "test_data = np.load(os.path.join(full_data_path, 'testset.npy'))\n",
        "\n",
        "print('Train data shape:', train_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucE3ca2PvNe9"
      },
      "outputs": [],
      "source": [
        "# PyTorch용 커스텀 데이터셋 클래스\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels=None, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.labels is not None:\n",
        "            label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "            return image, label\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "class TransformedDataset(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.subset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-C2oo7SvNe-"
      },
      "source": [
        "## 3. 최종 전략 설정 및 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlXY8LlOvNe-"
      },
      "outputs": [],
      "source": [
        "# --- 하이퍼파라미터 ---\n",
        "N_SPLITS = 5\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.1\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-4\n",
        "\n",
        "# K-Fold 설정\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
        "original_train_dataset = CustomDataset(train_data, train_labels)\n",
        "\n",
        "# --- 업그레이드: RandAugment 적용 ---\n",
        "cifar100_mean = (0.5071, 0.4867, 0.4408)\n",
        "cifar100_std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandAugment(), # <-- RandAugment\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar100_mean, cifar100_std),\n",
        "    transforms.RandomErasing(p=0.5)\n",
        "])\n",
        "\n",
        "transform_val_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar100_mean, cifar100_std)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAaxspfgvNe-"
      },
      "outputs": [],
      "source": [
        "# WideResNet 모델 정의\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, dropout_rate, stride=1):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=True)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=True))\n",
        "    def forward(self, x):\n",
        "        out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, dropout_rate, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "        assert ((depth - 4) % 6 == 0), 'Wide-resnet depth should be 6n+4'\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "        nStages = [16, 16 * k, 32 * k, 64 * k]\n",
        "        self.conv1 = nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=True)\n",
        "        self.layer1 = self._make_layer(WideBasicBlock, nStages[1], n, dropout_rate, stride=1)\n",
        "        self.layer2 = self._make_layer(WideBasicBlock, nStages[2], n, dropout_rate, stride=2)\n",
        "        self.layer3 = self._make_layer(WideBasicBlock, nStages[3], n, dropout_rate, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(nStages[3], momentum=0.9)\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "    def _make_layer(self, block, planes, num_blocks, dropout_rate, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, dropout_rate, stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jFa4JVuvNe_"
      },
      "source": [
        "## 4. 학습 루프 (Label Smoothing만 적용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3GNmaY6vNe_",
        "outputId": "8e95ffe5-b931-4bd8-a617-6db735bca778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== FOLD 1/5 ===============\n",
            "Epoch 10/200 | Val Acc: 0.2346 | LR: 0.099838\n",
            "Epoch 20/200 | Val Acc: 0.3651 | LR: 0.098547\n",
            "Epoch 30/200 | Val Acc: 0.3542 | LR: 0.095999\n",
            "Epoch 40/200 | Val Acc: 0.4485 | LR: 0.092260\n",
            "Epoch 50/200 | Val Acc: 0.4571 | LR: 0.087426\n",
            "Epoch 60/200 | Val Acc: 0.4183 | LR: 0.081622\n",
            "Epoch 70/200 | Val Acc: 0.4666 | LR: 0.075000\n",
            "Epoch 80/200 | Val Acc: 0.4756 | LR: 0.067730\n",
            "Epoch 90/200 | Val Acc: 0.4883 | LR: 0.060001\n",
            "Epoch 100/200 | Val Acc: 0.5191 | LR: 0.052013\n",
            "Epoch 110/200 | Val Acc: 0.5475 | LR: 0.043973\n",
            "Epoch 120/200 | Val Acc: 0.5355 | LR: 0.036089\n",
            "Epoch 130/200 | Val Acc: 0.5636 | LR: 0.028565\n",
            "Epoch 140/200 | Val Acc: 0.5576 | LR: 0.021597\n",
            "Epoch 150/200 | Val Acc: 0.5929 | LR: 0.015364\n",
            "Epoch 160/200 | Val Acc: 0.5943 | LR: 0.010028\n",
            "Epoch 170/200 | Val Acc: 0.6397 | LR: 0.005727\n",
            "Epoch 180/200 | Val Acc: 0.6679 | LR: 0.002573\n",
            "Epoch 190/200 | Val Acc: 0.6602 | LR: 0.000647\n",
            "Epoch 200/200 | Val Acc: 0.6640 | LR: 0.000000\n",
            "Fold 1 Best Val Acc: 0.6686\n",
            "Best model for Fold 1 saved to /content/drive/MyDrive/best_model_fold_0_comeback.pth\n",
            "=============== FOLD 2/5 ===============\n",
            "Epoch 10/200 | Val Acc: 0.2810 | LR: 0.099838\n",
            "Epoch 20/200 | Val Acc: 0.4345 | LR: 0.098547\n",
            "Epoch 30/200 | Val Acc: 0.4110 | LR: 0.095999\n",
            "Epoch 40/200 | Val Acc: 0.3928 | LR: 0.092260\n",
            "Epoch 50/200 | Val Acc: 0.4379 | LR: 0.087426\n",
            "Epoch 60/200 | Val Acc: 0.4810 | LR: 0.081622\n",
            "Epoch 70/200 | Val Acc: 0.4365 | LR: 0.075000\n",
            "Epoch 80/200 | Val Acc: 0.4820 | LR: 0.067730\n",
            "Epoch 90/200 | Val Acc: 0.4597 | LR: 0.060001\n",
            "Epoch 100/200 | Val Acc: 0.5317 | LR: 0.052013\n",
            "Epoch 110/200 | Val Acc: 0.5338 | LR: 0.043973\n",
            "Epoch 120/200 | Val Acc: 0.5212 | LR: 0.036089\n",
            "Epoch 130/200 | Val Acc: 0.5469 | LR: 0.028565\n",
            "Epoch 140/200 | Val Acc: 0.6070 | LR: 0.021597\n",
            "Epoch 150/200 | Val Acc: 0.6143 | LR: 0.015364\n",
            "Epoch 160/200 | Val Acc: 0.6198 | LR: 0.010028\n",
            "Epoch 170/200 | Val Acc: 0.6316 | LR: 0.005727\n",
            "Epoch 180/200 | Val Acc: 0.6724 | LR: 0.002573\n",
            "Epoch 190/200 | Val Acc: 0.6692 | LR: 0.000647\n",
            "Epoch 200/200 | Val Acc: 0.6765 | LR: 0.000000\n",
            "Fold 2 Best Val Acc: 0.6794\n",
            "Best model for Fold 2 saved to /content/drive/MyDrive/best_model_fold_1_comeback.pth\n",
            "=============== FOLD 3/5 ===============\n",
            "Epoch 10/200 | Val Acc: 0.2943 | LR: 0.099838\n",
            "Epoch 20/200 | Val Acc: 0.3458 | LR: 0.098547\n",
            "Epoch 30/200 | Val Acc: 0.4231 | LR: 0.095999\n",
            "Epoch 40/200 | Val Acc: 0.4047 | LR: 0.092260\n",
            "Epoch 50/200 | Val Acc: 0.4373 | LR: 0.087426\n",
            "Epoch 60/200 | Val Acc: 0.4427 | LR: 0.081622\n",
            "Epoch 70/200 | Val Acc: 0.4415 | LR: 0.075000\n",
            "Epoch 80/200 | Val Acc: 0.4789 | LR: 0.067730\n",
            "Epoch 90/200 | Val Acc: 0.4893 | LR: 0.060001\n",
            "Epoch 100/200 | Val Acc: 0.5136 | LR: 0.052013\n",
            "Epoch 110/200 | Val Acc: 0.4779 | LR: 0.043973\n",
            "Epoch 120/200 | Val Acc: 0.5196 | LR: 0.036089\n",
            "Epoch 130/200 | Val Acc: 0.5576 | LR: 0.028565\n",
            "Epoch 140/200 | Val Acc: 0.5815 | LR: 0.021597\n",
            "Epoch 150/200 | Val Acc: 0.6015 | LR: 0.015364\n",
            "Epoch 160/200 | Val Acc: 0.6191 | LR: 0.010028\n",
            "Epoch 170/200 | Val Acc: 0.6286 | LR: 0.005727\n",
            "Epoch 180/200 | Val Acc: 0.6646 | LR: 0.002573\n",
            "Epoch 190/200 | Val Acc: 0.6700 | LR: 0.000647\n",
            "Epoch 200/200 | Val Acc: 0.6746 | LR: 0.000000\n",
            "Fold 3 Best Val Acc: 0.6758\n",
            "Best model for Fold 3 saved to /content/drive/MyDrive/best_model_fold_2_comeback.pth\n",
            "=============== FOLD 4/5 ===============\n",
            "Epoch 10/200 | Val Acc: 0.3161 | LR: 0.099838\n",
            "Epoch 20/200 | Val Acc: 0.3597 | LR: 0.098547\n",
            "Epoch 30/200 | Val Acc: 0.4128 | LR: 0.095999\n",
            "Epoch 40/200 | Val Acc: 0.4327 | LR: 0.092260\n",
            "Epoch 50/200 | Val Acc: 0.4021 | LR: 0.087426\n",
            "Epoch 60/200 | Val Acc: 0.4436 | LR: 0.081622\n",
            "Epoch 70/200 | Val Acc: 0.4663 | LR: 0.075000\n",
            "Epoch 80/200 | Val Acc: 0.4759 | LR: 0.067730\n",
            "Epoch 90/200 | Val Acc: 0.4982 | LR: 0.060001\n",
            "Epoch 100/200 | Val Acc: 0.5216 | LR: 0.052013\n",
            "Epoch 110/200 | Val Acc: 0.5039 | LR: 0.043973\n",
            "Epoch 120/200 | Val Acc: 0.5634 | LR: 0.036089\n",
            "Epoch 130/200 | Val Acc: 0.5577 | LR: 0.028565\n",
            "Epoch 140/200 | Val Acc: 0.5514 | LR: 0.021597\n",
            "Epoch 150/200 | Val Acc: 0.5951 | LR: 0.015364\n",
            "Epoch 160/200 | Val Acc: 0.6191 | LR: 0.010028\n",
            "Epoch 170/200 | Val Acc: 0.6197 | LR: 0.005727\n",
            "Epoch 180/200 | Val Acc: 0.6587 | LR: 0.002573\n",
            "Epoch 190/200 | Val Acc: 0.6713 | LR: 0.000647\n",
            "Epoch 200/200 | Val Acc: 0.6789 | LR: 0.000000\n",
            "Fold 4 Best Val Acc: 0.6824\n",
            "Best model for Fold 4 saved to /content/drive/MyDrive/best_model_fold_3_comeback.pth\n",
            "=============== FOLD 5/5 ===============\n",
            "Epoch 10/200 | Val Acc: 0.3085 | LR: 0.099838\n",
            "Epoch 20/200 | Val Acc: 0.3910 | LR: 0.098547\n",
            "Epoch 30/200 | Val Acc: 0.3838 | LR: 0.095999\n",
            "Epoch 40/200 | Val Acc: 0.4137 | LR: 0.092260\n",
            "Epoch 50/200 | Val Acc: 0.4530 | LR: 0.087426\n",
            "Epoch 60/200 | Val Acc: 0.5001 | LR: 0.081622\n",
            "Epoch 70/200 | Val Acc: 0.4652 | LR: 0.075000\n",
            "Epoch 80/200 | Val Acc: 0.4477 | LR: 0.067730\n",
            "Epoch 90/200 | Val Acc: 0.5011 | LR: 0.060001\n",
            "Epoch 100/200 | Val Acc: 0.5311 | LR: 0.052013\n",
            "Epoch 110/200 | Val Acc: 0.4928 | LR: 0.043973\n",
            "Epoch 120/200 | Val Acc: 0.5455 | LR: 0.036089\n",
            "Epoch 130/200 | Val Acc: 0.5752 | LR: 0.028565\n",
            "Epoch 140/200 | Val Acc: 0.5266 | LR: 0.021597\n",
            "Epoch 150/200 | Val Acc: 0.5942 | LR: 0.015364\n",
            "Epoch 160/200 | Val Acc: 0.6280 | LR: 0.010028\n",
            "Epoch 170/200 | Val Acc: 0.6300 | LR: 0.005727\n",
            "Epoch 180/200 | Val Acc: 0.6557 | LR: 0.002573\n",
            "Epoch 190/200 | Val Acc: 0.6740 | LR: 0.000647\n",
            "Epoch 200/200 | Val Acc: 0.6775 | LR: 0.000000\n",
            "Fold 5 Best Val Acc: 0.6809\n",
            "Best model for Fold 5 saved to /content/drive/MyDrive/best_model_fold_4_comeback.pth\n",
            "\n",
            "Comeback strategy training finished!\n"
          ]
        }
      ],
      "source": [
        "# Label Smoothing Loss\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, smoothing=0.1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=-1)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=-1))\n",
        "\n",
        "# --- K-Fold 루프 시작 ---\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(original_train_dataset.data, original_train_dataset.labels)):\n",
        "    print(f'=============== FOLD {fold+1}/{N_SPLITS} ===============')\n",
        "\n",
        "    # --- WRN-16-2로 복귀 ---\n",
        "    model = WideResNet(depth=16, widen_factor=2, dropout_rate=0.3, num_classes=100).to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    criterion = LabelSmoothingLoss(classes=100, smoothing=0.1)\n",
        "    scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS-5)\n",
        "\n",
        "    # 데이터로더 생성\n",
        "    train_subset = Subset(original_train_dataset, train_idx)\n",
        "    val_subset = Subset(original_train_dataset, val_idx)\n",
        "    train_dataset_aug = TransformedDataset(train_subset, transform_train)\n",
        "    val_dataset_aug = TransformedDataset(val_subset, transform_val_test)\n",
        "    train_loader = DataLoader(train_dataset_aug, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset_aug, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    best_val_accuracy = 0.0\n",
        "    model_save_path = f'/content/drive/MyDrive/best_model_fold_{fold}_comeback.pth' # 새 버전으로 저장\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        # Warmup\n",
        "        if epoch < 5:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = LEARNING_RATE * (epoch + 1) / 5\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch >= 5: scheduler_cosine.step()\n",
        "\n",
        "        # 검증\n",
        "        model.eval()\n",
        "        val_corrects = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_corrects += torch.sum(preds == labels.data)\n",
        "        epoch_val_acc = val_corrects.double() / len(val_subset)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{EPOCHS} | Val Acc: {epoch_val_acc:.4f} | LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        if epoch_val_acc > best_val_accuracy:\n",
        "            best_val_accuracy = epoch_val_acc\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    print(f'Fold {fold+1} Best Val Acc: {best_val_accuracy:.4f}')\n",
        "    print(f'Best model for Fold {fold+1} saved to {model_save_path}')\n",
        "\n",
        "print('\\nComeback strategy training finished!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc8MKyhAvNe_"
      },
      "source": [
        "## 5. 앙상블 추론 및 최종 제출 (TTA 적용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "QdmK_O9HvNe_",
        "outputId": "32f2dfee-f5fb-4f0a-b259-636d454ddad4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CustomDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3469762509.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 테스트 데이터셋 정의\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_dataset_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_val_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_loader_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 5개의 Fold 모델들을 모두 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CustomDataset' is not defined"
          ]
        }
      ],
      "source": [
        "# 테스트 데이터셋 정의\n",
        "test_dataset_final = CustomDataset(test_data, labels=None, transform=transform_val_test)\n",
        "test_loader_final = DataLoader(test_dataset_final, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 5개의 Fold 모델들을 모두 로드\n",
        "models = []\n",
        "for fold in range(N_SPLITS):\n",
        "    model_path = f'/content/drive/MyDrive/best_model_fold_{fold}_comeback.pth'\n",
        "    model = WideResNet(depth=16, widen_factor=2, dropout_rate=0.3, num_classes=100).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    models.append(model)\n",
        "print(f'{len(models)} models loaded for ensemble.')\n",
        "\n",
        "# TTA 적용 앙상블 추론\n",
        "tta_transform = transforms.RandomHorizontalFlip(p=1.0)\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for images in test_loader_final:\n",
        "        images = images.to(device)\n",
        "        ensemble_outputs = torch.zeros(images.size(0), 100).to(device)\n",
        "\n",
        "        # 원본 이미지 예측\n",
        "        for model in models:\n",
        "            outputs = model(images)\n",
        "            ensemble_outputs += F.softmax(outputs, dim=1)\n",
        "\n",
        "        # 좌우 반전된 이미지 예측\n",
        "        flipped_images = tta_transform(images)\n",
        "        for model in models:\n",
        "            outputs = model(flipped_images)\n",
        "            ensemble_outputs += F.softmax(outputs, dim=1)\n",
        "\n",
        "        _, preds = torch.max(ensemble_outputs, 1)\n",
        "        all_predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "print(\"Ensemble prediction with TTA finished!\")\n",
        "\n",
        "# 최종 제출 파일 생성\n",
        "sample_submission_path = os.path.join(full_data_path, 'sample_submission.csv')\n",
        "submission_df = pd.read_csv(sample_submission_path)\n",
        "submission_df['label'] = all_predictions\n",
        "submission_file_path = '/content/submission_comeback.csv'\n",
        "submission_df.to_csv(submission_file_path, index=False)\n",
        "\n",
        "print(f\"Final submission file created at: {submission_file_path}\")\n",
        "print(\"\\n--- Final Submission File Preview ---\")\n",
        "print(submission_df.head())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}